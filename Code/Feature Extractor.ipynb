{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Partha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Partha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Partha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "import ntpath\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from string import punctuation\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "allfiles=glob.glob(\"C:/Users/Partha/Downloads/Task/test docs/*.tx?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code finds out the list of most important most common words from a list of documents and writes it to a json file in the following format into a json which can be used from other web applications.\n",
    "Table format \n",
    "Keyword | Docs | Sentences\n",
    "\n",
    "Following steps summarize the tasks:\n",
    "1) Read the text from the folder into doc _corpus\n",
    "2) Perform the following steps to clean up the data\n",
    "   a)Tokenize the sentences into words.\n",
    "   b)Remove stopwords and irrelevant words like pronouns, as they wouldn't be good hash tag candidates.\n",
    "   c) Use lemmatizer to group together different forms of the word to mean the same term like two and 2 should be considered the same in an NLP problem which filters the most important word.\n",
    "   d) Use Stemmer to reduce reocurring verb forms to the root form like say,saying said gets reduced to say\n",
    "   \n",
    "3) Use Count Vectorizer to get a matrix with word  counts across the Corpus and find out relevant features.\n",
    "4) Use TF IDF to find word relevance so that the frequency of the word in a document is compared to frquency of the word across the documents, this filters out irrelevant words by assigning them a low relevance score.\n",
    "5) Combine the output features from count vectorizer and relevance matrix from TF IDF to generate the top 10 keywords\n",
    "6) Create a Table to display the documents containing the keywords and filter out the sentences.\n",
    "7) Convert output to a dictionary and write it to a JSON file.\n",
    "   \n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the required packages\n",
    "import nltk\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "import ntpath\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from string import punctuation\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing Variable and instantiating classes.\n",
    "doc_corpus =pd.DataFrame()\n",
    "filtered_data=pd.DataFrame()\n",
    "name_list=[]\n",
    "name_list_new=[]\n",
    "doc_list=[]\n",
    "sentence_list =[]\n",
    "content = []\n",
    "sent=[]\n",
    "b=[]\n",
    "\n",
    "stop_words = set( stopwords.words('english') + list(punctuation) + list(['good','let','how', 'mama','who','why','yo','\\n'])+['reuter', '\\x03'])\n",
    "cv=CountVectorizer(stop_words=stop_words)\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=.3,max_features=10000,lowercase=True, analyzer='word',stop_words= 'english',ngram_range=(1,1))\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "porter = PorterStemmer()\n",
    "tfid_trans=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "stop_words = set( stopwords.words('english') +  list(['good','let','who','why','how','\\n'])+['reuter', '\\x03'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions required for pre and post processing\n",
    "def read_files() :\n",
    "    #read all files from the folder and concatenate output into one list of texts\n",
    "    allfiles=glob.glob(\"C:/Users/Partha/Downloads/Task/test docs/*.tx?\")           \n",
    "    for fname in allfiles:\n",
    "     name_list.append(str(ntpath.basename(fname)))\n",
    "     with open(fname,\"r\",encoding='utf-8-sig') as f:\n",
    "      b=[str(f.read().splitlines())]\n",
    "      sent.append(sent_tokenize(str(b)))\n",
    "      content.append(b) \n",
    "    doc_corpus['name'] =name_list\n",
    "    doc_corpus['content'] = content\n",
    "    return (content)    \n",
    "\n",
    "def tokenize(text):\n",
    "    #tokenize the sentence into words and ignore unnecessary common words like pronouns \n",
    "    words=re.findall('[\\w]+', text.lower())\n",
    "    answer=[w for w in words if w not in stop_words and not w.isdigit()]\n",
    "    return(answer)\n",
    "\n",
    "def data_clean(text):\n",
    "    #Adding code to clean data and remove words that are not ideal hashtag candidates\n",
    "    token_text=[str(tokenize(str(content)))]\n",
    "    lemma_txt=[str(lemmatizer.lemmatize(str(token_text)))]\n",
    "    final_txt=[str(porter.stem(str(lemma_txt)))]\n",
    "    return(final_txt) \n",
    "\n",
    "def sort_coo (coo_vector):\n",
    "    # sortting the matrix containing the relevance of all words \n",
    "    tuples=zip(coo_vector.col,coo_vector.data)\n",
    "    return(sorted(tuples,key=lambda x :(x[1],x[0]),reverse=True))\n",
    "           \n",
    "def extract_top10 (feat,sorted_coo):\n",
    "    #filter out the top 10 most important matrices\n",
    "    sorted_coo=sorted_coo[:10]\n",
    "    score_v = []\n",
    "    feat_value =[]\n",
    "    for idx, score in sorted_coo:\n",
    "        fname = feat[idx]\n",
    "        #store feature name and its relevance score\n",
    "        score_v.append(round(score, 3))\n",
    "        feat_value.append(feat[idx])\n",
    "\n",
    "    #create a tuples of feature,score\n",
    "    results= {}\n",
    "    for idx in range(len(feat_value)):\n",
    "        results[feat_value[idx]]=score_v[idx]\n",
    "    \n",
    "    return results\n",
    " \n",
    "def search_keyword(keywords,doc_corpus):\n",
    " # Lookup the doc_corpus to find the documents and sentences containing the keyword   \n",
    " filtered_data=pd.DataFrame()\n",
    " for keyword in keywords.keys() :\n",
    "   for idx,row in doc_corpus.iterrows():\n",
    "    for line in str(row['content']).splitlines():\n",
    "       words =set(tokenize(line))\n",
    "       if keyword in words:\n",
    "            name_list_new.append(row['name'])\n",
    "            sentence_list.append(line)        \n",
    "   mod_data=filtered_data.append({'keyword' :keyword, 'name' : str(name_list_new),'line':str(sentence_list)},ignore_index=True)\n",
    "   filtered_data=mod_data\n",
    " return(mod_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count Vectorizer and TF IDF at work\n",
    "content=read_files()\n",
    "final_txt=data_clean(content)\n",
    "#Running count vectorizer on tokenized textto find the word counts\n",
    "wordcount =cv.fit_transform(final_txt)\n",
    "#The get_feature_names attribute gives the distinct important words in the text corpus\n",
    "feat=cv.get_feature_names()\n",
    "#Using a TF IDF Vectorizer to find the relation of the word frequency in relation to the frequenct of word across documents\n",
    "tfid_trans.fit(wordcount)\n",
    "tfidf_vec=tfid_trans.transform(cv.transform(final_txt))\n",
    "#Following code is used to sort the matrix containing the relevance score of each word generated as a feature by Count Vectorizer\n",
    "sorted_items=sort_coo(tfidf_vec.tocoo())\n",
    "#Extracting top 10 keywords\n",
    "keywords= extract_top10 (feat,sorted_items)\n",
    "#Search the  documents and the sentences containing most important keywords\n",
    "filtered_data=search_keyword(keywords,doc_corpus)\n",
    "#Writing the collected data into a JSON file, which can be imported by other applications.\n",
    "with open(\"file_output.txt\",'w+') as json_file:\n",
    "  json.dump(filtered_data.to_dict(),json_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
